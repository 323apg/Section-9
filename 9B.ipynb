{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550cc534-39ce-433a-a8f1-57af0cb1c3ed",
   "metadata": {},
   "source": [
    "1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7761837-126e-4654-bbd8-c303e373664a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | MSE Loss: 1.4333\n",
      "Epoch 10 | MSE Loss: 1.4225\n",
      "Epoch 20 | MSE Loss: 1.4137\n",
      "Epoch 30 | MSE Loss: 1.4064\n",
      "Epoch 40 | MSE Loss: 1.4005\n",
      "Epoch 49 | MSE Loss: 1.3961\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class FourLayerMLP:\n",
    "    \"\"\"\n",
    "    A simple 4-layer MLP with fully connected layers:\n",
    "      - Layer 1: Input -> Hidden1\n",
    "      - Layer 2: Hidden1 -> Hidden2\n",
    "      - Layer 3: Hidden2 -> Hidden3\n",
    "      - Layer 4: Hidden3 -> Output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, h1_dim, h2_dim, h3_dim, output_dim, activation='relu'):\n",
    "        \"\"\"\n",
    "        input_dim:  size of input features\n",
    "        h1_dim:     number of neurons in hidden layer 1\n",
    "        h2_dim:     number of neurons in hidden layer 2\n",
    "        h3_dim:     number of neurons in hidden layer 3\n",
    "        output_dim: number of output neurons\n",
    "        activation: 'relu' or 'sigmoid'\n",
    "        \"\"\"\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weight & bias parameters\n",
    "        # We have 4 sets of weights/biases:\n",
    "        #   W1, b1 -> shape (input_dim, h1_dim)\n",
    "        #   W2, b2 -> shape (h1_dim, h2_dim)\n",
    "        #   W3, b3 -> shape (h2_dim, h3_dim)\n",
    "        #   W4, b4 -> shape (h3_dim, output_dim)\n",
    "        self.W1 = 0.01 * np.random.randn(input_dim, h1_dim)\n",
    "        self.b1 = np.zeros((1, h1_dim))\n",
    "        self.W2 = 0.01 * np.random.randn(h1_dim, h2_dim)\n",
    "        self.b2 = np.zeros((1, h2_dim))\n",
    "        self.W3 = 0.01 * np.random.randn(h2_dim, h3_dim)\n",
    "        self.b3 = np.zeros((1, h3_dim))\n",
    "        self.W4 = 0.01 * np.random.randn(h3_dim, output_dim)\n",
    "        self.b4 = np.zeros((1, output_dim))\n",
    "\n",
    "    def _activate(self, z, derivative=False):\n",
    "        \"\"\"\n",
    "        Applies the activation function (ReLU or Sigmoid).\n",
    "        If derivative=True, returns the local gradient w.r.t. z.\n",
    "        \"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            if derivative:\n",
    "                return (z > 0).astype(float)\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            s = 1.0 / (1.0 + np.exp(-z))\n",
    "            if derivative:\n",
    "                return s * (1.0 - s)\n",
    "            return s\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "          Z1 = X W1 + b1   -> A1 = activate(Z1)\n",
    "          Z2 = A1 W2 + b2  -> A2 = activate(Z2)\n",
    "          Z3 = A2 W3 + b3  -> A3 = activate(Z3)\n",
    "          Z4 = A3 W4 + b4  -> Output = A4\n",
    "        \"\"\"\n",
    "        self.X0 = X  # store input\n",
    "        self.Z1 = X @ self.W1 + self.b1\n",
    "        self.A1 = self._activate(self.Z1)\n",
    "\n",
    "        self.Z2 = self.A1 @ self.W2 + self.b2\n",
    "        self.A2 = self._activate(self.Z2)\n",
    "\n",
    "        self.Z3 = self.A2 @ self.W3 + self.b3\n",
    "        self.A3 = self._activate(self.Z3)\n",
    "\n",
    "        self.Z4 = self.A3 @ self.W4 + self.b4\n",
    "        self.A4 = self.Z4  # Often final layer is linear; or apply an activation if desired\n",
    "\n",
    "        return self.A4\n",
    "\n",
    "    def backward(self, X, Y, lr=0.001):\n",
    "        \"\"\"\n",
    "        Backpropagation for MSE loss:\n",
    "          dLoss/dA4 = A4 - Y\n",
    "          Then we propagate through each layer in reverse.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # batch size\n",
    "        dA4 = (self.A4 - Y)  # dLoss/dA4\n",
    "\n",
    "        # Layer 4\n",
    "        dZ4 = dA4  # if no activation on final output\n",
    "        dW4 = (self.A3.T @ dZ4) / m\n",
    "        db4 = np.sum(dZ4, axis=0, keepdims=True) / m\n",
    "\n",
    "        # backprop to layer 3\n",
    "        dA3 = dZ4 @ self.W4.T\n",
    "        dZ3 = dA3 * self._activate(self.Z3, derivative=True)\n",
    "        dW3 = (self.A2.T @ dZ3) / m\n",
    "        db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "\n",
    "        # backprop to layer 2\n",
    "        dA2 = dZ3 @ self.W3.T\n",
    "        dZ2 = dA2 * self._activate(self.Z2, derivative=True)\n",
    "        dW2 = (self.A1.T @ dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "        # backprop to layer 1\n",
    "        dA1 = dZ2 @ self.W2.T\n",
    "        dZ1 = dA1 * self._activate(self.Z1, derivative=True)\n",
    "        dW1 = (self.X0.T @ dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Update parameters\n",
    "        self.W4 -= lr * dW4\n",
    "        self.b4 -= lr * db4\n",
    "        self.W3 -= lr * dW3\n",
    "        self.b3 -= lr * db3\n",
    "        self.W2 -= lr * dW2\n",
    "        self.b2 -= lr * db2\n",
    "        self.W1 -= lr * dW1\n",
    "        self.b1 -= lr * db1\n",
    "\n",
    "    def train(self, X, Y, epochs=100, lr=0.001, verbose=True):\n",
    "        \"\"\"\n",
    "        Train with gradient descent for 'epochs' passes over data (X, Y).\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, Y, lr=lr)\n",
    "            if verbose and (epoch % 10 == 0 or epoch == epochs-1):\n",
    "                loss = np.mean((self.A4 - Y)**2)\n",
    "                print(f\"Epoch {epoch} | MSE Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "def demo():\n",
    "    # Example usage:\n",
    "    # We want a 4-layer MLP: input-> h1-> h2-> h3-> output\n",
    "    net = FourLayerMLP(input_dim=2, h1_dim=3, h2_dim=4, h3_dim=3, output_dim=1, activation='relu')\n",
    "\n",
    "    # Fake data: X in R^2, Y in R^1\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(100, 2)\n",
    "    Y = (X[:, 0:1]**2 + 0.5 * X[:, 1:2] - 1.0)  # some arbitrary target\n",
    "\n",
    "    net.train(X, Y, epochs=50, lr=0.01, verbose=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214623f0-8839-46e1-a804-616134b7f1ff",
   "metadata": {},
   "source": [
    "1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3289a5-0217-48a5-800f-a58a294c1e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, MSE Loss: 0.29671\n",
      "Epoch 10, MSE Loss: 0.29264\n",
      "Epoch 20, MSE Loss: 0.28866\n",
      "Epoch 30, MSE Loss: 0.28479\n",
      "Epoch 40, MSE Loss: 0.28101\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    A simple MLP that supports multiple layers with either ReLU or Sigmoid activation.\n",
    "    Uses NumPy for forward and backward propagation, illustrating the use of np.einsum\n",
    "    for outer products in the backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer_sizes, activation='relu'):\n",
    "        \"\"\"\n",
    "        layer_sizes: list of layer dimensions, e.g. [input_dim, hidden1_dim, hidden2_dim, output_dim].\n",
    "        activation:  'relu' or 'sigmoid'.\n",
    "        \"\"\"\n",
    "        self.activation = activation\n",
    "        self.num_layers = len(layer_sizes) - 1\n",
    "        # Initialize weights and biases\n",
    "        # Weights[i] has shape (layer_sizes[i], layer_sizes[i+1])\n",
    "        # Biases[i]   has shape (layer_sizes[i+1], )\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.num_layers):\n",
    "            w = 0.01 * np.random.randn(layer_sizes[i], layer_sizes[i+1])\n",
    "            b = np.zeros(layer_sizes[i+1])\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def _activate(self, z, derivative=False):\n",
    "        \"\"\"\n",
    "        Applies either ReLU or Sigmoid. If derivative=True, returns derivative w.r.t. z.\n",
    "        \"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            if derivative:\n",
    "                return (z > 0).astype(float)\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            s = 1.0 / (1.0 + np.exp(-z))\n",
    "            if derivative:\n",
    "                return s * (1.0 - s)\n",
    "            return s\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through all layers. Store pre-activation (Z) and activation (A) for each layer.\n",
    "        A[0] = X (input), A[L] = final output.\n",
    "        \"\"\"\n",
    "        self.A = [X]          # Activations for each layer\n",
    "        self.Z = []           # Pre-activations for each layer\n",
    "        current = X\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # Z_i = A_{i} * W_i + b_i\n",
    "            # shape of current: (batch_size, layer_sizes[i])\n",
    "            # shape of weights[i]: (layer_sizes[i], layer_sizes[i+1])\n",
    "            z_i = current @ self.weights[i] + self.biases[i]\n",
    "            self.Z.append(z_i)\n",
    "            # Activation\n",
    "            a_i = self._activate(z_i)\n",
    "            self.A.append(a_i)\n",
    "            current = a_i\n",
    "\n",
    "        return self.A[-1]  # final layer activation\n",
    "\n",
    "    def backward(self, Y, lr=0.01):\n",
    "        \"\"\"\n",
    "        Backprop for MSE loss: loss = 0.5 * sum((A[L] - Y)^2).\n",
    "        We'll compute dLoss/dA[L], then propagate backwards.\n",
    "        Use np.einsum to form outer products for weight updates.\n",
    "        \n",
    "        Y shape: (batch_size, layer_sizes[-1])\n",
    "        \"\"\"\n",
    "        batch_size = Y.shape[0]\n",
    "        # Output layer error: dLoss/dA[L] = (A[L] - Y)\n",
    "        dA = (self.A[-1] - Y)\n",
    "\n",
    "        # We'll go layer by layer in reverse\n",
    "        for i in reversed(range(self.num_layers)):\n",
    "            # dZ = dA * activation'(Z)\n",
    "            dZ = dA * self._activate(self.Z[i], derivative=True)\n",
    "\n",
    "            # Weight gradient: dW = A[i]^T * dZ  (batched)\n",
    "            # But let's do it with np.einsum to demonstrate usage:\n",
    "            # shape(A[i]) = (batch_size, layer_sizes[i])\n",
    "            # shape(dZ)   = (batch_size, layer_sizes[i+1])\n",
    "            # We want outer sum => (layer_sizes[i], layer_sizes[i+1])\n",
    "            dW = np.zeros_like(self.weights[i])\n",
    "            db = np.sum(dZ, axis=0)\n",
    "\n",
    "            # For each sample in the batch:\n",
    "            for b_idx in range(batch_size):\n",
    "                # outer product of A[i][b_idx,:] and dZ[b_idx,:]\n",
    "                dW += np.einsum('i,j->ij', self.A[i][b_idx], dZ[b_idx])\n",
    "            dW /= batch_size\n",
    "            db /= batch_size\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= lr * dW\n",
    "            self.biases[i]  -= lr * db\n",
    "\n",
    "            # Compute dA for next layer if not the first layer\n",
    "            if i > 0:\n",
    "                # dA_{i-1} = dZ * W_i^T\n",
    "                # shape(dZ) = (batch_size, layer_sizes[i+1])\n",
    "                # shape(W_i^T) = (layer_sizes[i+1], layer_sizes[i])\n",
    "                dA = dZ @ self.weights[i].T\n",
    "\n",
    "    def train(self, X, Y, epochs=100, lr=0.01, verbose=True):\n",
    "        \"\"\"\n",
    "        Simple training loop: forward -> backward -> update.\n",
    "        X shape: (batch_size, input_dim)\n",
    "        Y shape: (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        for e in range(epochs):\n",
    "            # Forward\n",
    "            pred = self.forward(X)\n",
    "            # Compute MSE\n",
    "            loss = np.mean(0.5 * (pred - Y)**2)\n",
    "            # Backprop\n",
    "            self.backward(Y, lr=lr)\n",
    "\n",
    "            if verbose and e % 10 == 0:\n",
    "                print(f\"Epoch {e}, MSE Loss: {loss:.5f}\")\n",
    "\n",
    "\n",
    "def demo():\n",
    "    # Example usage:\n",
    "    # We'll create a small MLP with 1 hidden layer [2 -> 4 -> 1]\n",
    "    # or 2 hidden layers if we like, e.g. [2 -> 4 -> 4 -> 1].\n",
    "    layer_sizes = [2, 4, 1]\n",
    "    net = MLP(layer_sizes, activation='sigmoid')\n",
    "\n",
    "    # Create dummy data: input X of shape (100, 2), Y of shape (100, 1)\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(100, 2)\n",
    "    Y = (X[:, :1] * 0.5 + X[:, 1:] * (-0.3))  # some linear combination\n",
    "\n",
    "    # Train\n",
    "    net.train(X, Y, epochs=50, lr=0.01, verbose=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3645f-ed07-4529-8a5f-e2f9b9855125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb182eff-bdb4-421b-9167-3015e3db354c",
   "metadata": {},
   "source": [
    "2A 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9efceb-029e-4759-89e1-1218656a1dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 0.47075\n",
      "Epoch 1, Loss = 0.46744\n",
      "Epoch 2, Loss = 0.46419\n",
      "Epoch 3, Loss = 0.46101\n",
      "Epoch 4, Loss = 0.45789\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    s = 1.0 / (1.0 + np.exp(-x))\n",
    "    if derivative:\n",
    "        return s * (1.0 - s)\n",
    "    return s\n",
    "\n",
    "class SimpleCNN:\n",
    "    \"\"\"\n",
    "    A minimal CNN with:\n",
    "      - One convolutional layer (with a small number of filters).\n",
    "      - One fully connected layer for classification/regression.\n",
    "      - Supports ReLU or Sigmoid activation for both layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_channels=1, \n",
    "                 out_channels=2, \n",
    "                 kernel_size=3, \n",
    "                 fc_size=4, \n",
    "                 num_classes=2,\n",
    "                 activation='relu'):\n",
    "        \"\"\"\n",
    "        in_channels:   number of channels in the input image (e.g. 1 for grayscale, 3 for RGB).\n",
    "        out_channels:  number of filters in the conv layer.\n",
    "        kernel_size:   size of each filter (assume square).\n",
    "        fc_size:       number of hidden units in the fully connected layer.\n",
    "        num_classes:   output dimension (e.g. number of classes).\n",
    "        activation:    'relu' or 'sigmoid'.\n",
    "        \"\"\"\n",
    "        self.activation_name = activation\n",
    "        if activation == 'relu':\n",
    "            self.act_fn = relu\n",
    "        else:\n",
    "            self.act_fn = sigmoid\n",
    "\n",
    "        # Convolution filter weights: shape (out_channels, in_channels, kernel_size, kernel_size)\n",
    "        # Bias for each filter: shape (out_channels,)\n",
    "        self.conv_w = 0.01 * np.random.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        self.conv_b = np.zeros((out_channels,))\n",
    "\n",
    "        # We won't know the conv output shape until we see an input. Suppose we do 'valid' conv.\n",
    "        # We'll do a lazy init for the fully connected layer once we see the input shape in forward().\n",
    "\n",
    "        self.fc_initialized = False\n",
    "        self.fc_w = None  # will be (flattened_dim, fc_size)\n",
    "        self.fc_b = None\n",
    "        self.fc_w2 = None # final layer (fc_size, num_classes)\n",
    "        self.fc_b2 = None\n",
    "        self.num_classes = num_classes\n",
    "        self.fc_size = fc_size\n",
    "\n",
    "    def _conv2d_forward(self, x):\n",
    "        \"\"\"\n",
    "        Naive forward pass for a 'valid' convolution (no padding, stride=1).\n",
    "        x shape: (batch_size, in_channels, H, W)\n",
    "        returns: (batch_size, out_channels, H_out, W_out)\n",
    "        \"\"\"\n",
    "        batch_size, in_c, H, W = x.shape\n",
    "        out_c, _, K, _ = self.conv_w.shape\n",
    "        H_out = H - K + 1\n",
    "        W_out = W - K + 1\n",
    "\n",
    "        # Output\n",
    "        out = np.zeros((batch_size, out_c, H_out, W_out))\n",
    "\n",
    "        # Convolution loop (naive)\n",
    "        for n in range(batch_size):\n",
    "            for oc in range(out_c):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        # region from x\n",
    "                        patch = x[n, :, i:i+K, j:j+K]  # shape (in_c, K, K)\n",
    "                        # elementwise multiply with conv_w[oc], then sum\n",
    "                        out[n, oc, i, j] = np.sum(patch * self.conv_w[oc]) + self.conv_b[oc]\n",
    "        return out\n",
    "\n",
    "    def _conv2d_backward(self, x, d_out):\n",
    "        \"\"\"\n",
    "        Naive backward pass for conv layer. \n",
    "        x shape: (batch_size, in_channels, H, W)\n",
    "        d_out shape: (batch_size, out_channels, H_out, W_out) => gradient wrt conv output\n",
    "        returns:\n",
    "          dx: gradient wrt input x\n",
    "          dW: gradient wrt conv_w\n",
    "          db: gradient wrt conv_b\n",
    "        \"\"\"\n",
    "        batch_size, in_c, H, W = x.shape\n",
    "        out_c, _, K, _ = self.conv_w.shape\n",
    "        _, _, H_out, W_out = d_out.shape\n",
    "\n",
    "        dx = np.zeros_like(x)\n",
    "        dW = np.zeros_like(self.conv_w)\n",
    "        db = np.zeros_like(self.conv_b)\n",
    "\n",
    "        # Loop\n",
    "        for n in range(batch_size):\n",
    "            for oc in range(out_c):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        grad_val = d_out[n, oc, i, j]\n",
    "                        # This grad_val contributes to each element in patch\n",
    "                        dx[n, :, i:i+K, j:j+K] += self.conv_w[oc] * grad_val\n",
    "                        dW[oc] += x[n, :, i:i+K, j:j+K] * grad_val\n",
    "                        db[oc] += grad_val\n",
    "        return dx, dW, db\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "         1) Convolution -> activation\n",
    "         2) Flatten\n",
    "         3) Fully connected layer -> activation\n",
    "         4) Fully connected layer -> final output (logits or regression)\n",
    "        \"\"\"\n",
    "        self.x_in = x\n",
    "        # 1) Conv\n",
    "        self.z_conv = self._conv2d_forward(x)\n",
    "        self.a_conv = self.act_fn(self.z_conv)\n",
    "\n",
    "        # 2) Flatten\n",
    "        self.batch_size = x.shape[0]\n",
    "        self.conv_out_shape = self.a_conv.shape  # (batch, out_c, H_out, W_out)\n",
    "        flat_dim = np.prod(self.conv_out_shape[1:])  # out_c*H_out*W_out\n",
    "\n",
    "        self.a_flat = self.a_conv.reshape(self.batch_size, flat_dim)\n",
    "\n",
    "        # Lazy init for FC layers if needed\n",
    "        if not self.fc_initialized:\n",
    "            # first FC\n",
    "            self.fc_w = 0.01 * np.random.randn(flat_dim, self.fc_size)\n",
    "            self.fc_b = np.zeros((self.fc_size,))\n",
    "            # second FC\n",
    "            self.fc_w2 = 0.01 * np.random.randn(self.fc_size, self.num_classes)\n",
    "            self.fc_b2 = np.zeros((self.num_classes,))\n",
    "            self.fc_initialized = True\n",
    "\n",
    "        # 3) First FC\n",
    "        self.z_fc = self.a_flat @ self.fc_w + self.fc_b  # shape (batch_size, fc_size)\n",
    "        self.a_fc = self.act_fn(self.z_fc)\n",
    "\n",
    "        # 4) Second FC (final)\n",
    "        self.z_out = self.a_fc @ self.fc_w2 + self.fc_b2  # shape (batch_size, num_classes)\n",
    "        # If classification, might want softmax here. We'll keep it linear for demonstration.\n",
    "        self.out = self.z_out\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, d_out, lr=0.001):\n",
    "        \"\"\"\n",
    "        Backprop:\n",
    "          d_out shape: (batch_size, num_classes) => gradient w.r.t. final output\n",
    "        We'll propagate all the way back to the input x.\n",
    "        \"\"\"\n",
    "        # 1) Backprop final FC: out = a_fc @ fc_w2 + fc_b2\n",
    "        d_a_fc = d_out @ self.fc_w2.T\n",
    "        d_w2 = self.a_fc.T @ d_out\n",
    "        d_b2 = np.sum(d_out, axis=0)\n",
    "\n",
    "        # 2) Backprop first FC: a_fc = activation(z_fc), z_fc = a_flat @ fc_w + fc_b\n",
    "        d_z_fc = d_a_fc * self.act_fn(self.z_fc, derivative=True)\n",
    "        d_a_flat = d_z_fc @ self.fc_w.T\n",
    "        d_w = self.a_flat.T @ d_z_fc\n",
    "        d_b = np.sum(d_z_fc, axis=0)\n",
    "\n",
    "        # 3) Reshape d_a_flat back to conv output shape\n",
    "        d_a_conv = d_a_flat.reshape(self.conv_out_shape)\n",
    "\n",
    "        # 4) Backprop activation from conv layer: a_conv = activation(z_conv)\n",
    "        d_z_conv = d_a_conv * self.act_fn(self.z_conv, derivative=True)\n",
    "\n",
    "        # 5) Backprop conv: x_in -> z_conv\n",
    "        dx_conv, d_conv_w, d_conv_b = self._conv2d_backward(self.x_in, d_z_conv)\n",
    "\n",
    "        # Update parameters\n",
    "        self.fc_w2 -= lr * d_w2\n",
    "        self.fc_b2 -= lr * d_b2\n",
    "        self.fc_w  -= lr * d_w\n",
    "        self.fc_b  -= lr * d_b\n",
    "        self.conv_w -= lr * d_conv_w\n",
    "        self.conv_b -= lr * d_conv_b\n",
    "\n",
    "        return dx_conv  # in case we want the gradient wrt input\n",
    "\n",
    "    def train(self, x, y, epochs=10, lr=0.001):\n",
    "        \"\"\"\n",
    "        A simple training loop for demonstration. We'll assume MSE loss or something similar.\n",
    "        y shape: (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        for e in range(epochs):\n",
    "            out = self.forward(x)\n",
    "            # Suppose we do MSE loss: L = 0.5 * sum((out - y)^2)\n",
    "            loss = 0.5 * np.mean((out - y)**2)\n",
    "            # d_out = (out - y) for MSE\n",
    "            d_out = (out - y) / x.shape[0]  # average over batch\n",
    "\n",
    "            self.backward(d_out, lr=lr)\n",
    "            if e % 1 == 0:\n",
    "                print(f\"Epoch {e}, Loss = {loss:.5f}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Demo usage\n",
    "# -------------------------------\n",
    "def demo():\n",
    "    # Create a random 'image' dataset: e.g. 10 images, single-channel, 6x6\n",
    "    np.random.seed(42)\n",
    "    x_data = np.random.randn(10, 1, 6, 6)  # (batch_size=10, in_channels=1, H=6, W=6)\n",
    "    # Let's say we want 2-class output => y shape = (10, 2)\n",
    "    y_data = np.random.randn(10, 2)\n",
    "\n",
    "    # Create a SimpleCNN: 1 conv layer -> 1 hidden FC -> final output\n",
    "    net = SimpleCNN(in_channels=1, out_channels=2, kernel_size=3,\n",
    "                    fc_size=4, num_classes=2, activation='relu')\n",
    "\n",
    "    net.train(x_data, y_data, epochs=5, lr=0.01)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e281b-26c4-43e2-b21d-a177a383242e",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce11bbad-abd6-46e0-b896-9ce9e51ea66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is a sample Notebook to demonstrate how to read \"MNIST Dataset\"\n",
    "#\n",
    "import numpy as np # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19bba294-bb61-4f27-8feb-8f77a2520845",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/train-labels-idx1-ubyte/train-labels-idx1-ubyte'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Load MINST dataset\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     37\u001b[0m mnist_dataloader \u001b[38;5;241m=\u001b[39m MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n\u001b[0;32m---> 38\u001b[0m (x_train, y_train), (x_test, y_test) \u001b[38;5;241m=\u001b[39m \u001b[43mmnist_dataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Show some random training and test images \u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     43\u001b[0m images_2_show \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[12], line 44\u001b[0m, in \u001b[0;36mMnistDataloader.load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 44\u001b[0m     x_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_images_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_images_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_labels_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     x_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_images_labels(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_images_filepath, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_labels_filepath)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (x_train, y_train),(x_test, y_test)\n",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m, in \u001b[0;36mMnistDataloader.read_images_labels\u001b[0;34m(self, images_filepath, labels_filepath)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_images_labels\u001b[39m(\u001b[38;5;28mself\u001b[39m, images_filepath, labels_filepath):        \n\u001b[1;32m     21\u001b[0m     labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     23\u001b[0m         magic, size \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>II\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2049\u001b[39m:\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/train-labels-idx1-ubyte/train-labels-idx1-ubyte'"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Verify Reading Dataset via MnistDataloader class\n",
    "#\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = '../input'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "#\n",
    "# Show some random training and test images \n",
    "#\n",
    "images_2_show = []\n",
    "titles_2_show = []\n",
    "for i in range(0, 10):\n",
    "    r = random.randint(1, 60000)\n",
    "    images_2_show.append(x_train[r])\n",
    "    titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))    \n",
    "\n",
    "for i in range(0, 5):\n",
    "    r = random.randint(1, 10000)\n",
    "    images_2_show.append(x_test[r])        \n",
    "    titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))    \n",
    "\n",
    "show_images(images_2_show, titles_2_show)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a60809-a851-4605-9cd7-3d82a0b12ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e9033-497d-42db-af3c-11c86177b9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
